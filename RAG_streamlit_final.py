import time
import warnings

from openai import OpenAI
from pinecone import Pinecone
from tqdm.auto import tqdm
from langchain_community.document_loaders import PyPDFLoader
from PyPDF2 import PdfReader
import streamlit as st
from openai import OpenAI
from pinecone import Pinecone   

warnings.filterwarnings('ignore')

# Function to process PDF
def process_pdf(file_path):
    loader = PyPDFLoader(file_path)
    data = loader.load()
    return data

# Function to get embeddings
def get_embeddings(text, model="text-embedding-ada-002"):
    text = text.replace("\n", " ")
    return openai_client.embeddings.create(input = text, model=model)

st.title("Resume QA with GPT-3.5 Turbo and Pinecone")

# Sidebar Inputs for API Keys
st.sidebar.title("Enter API Keys")
openai_api_key = st.sidebar.text_input("OpenAI API Key", type='password')
pinecone_api_key = st.sidebar.text_input("Pinecone API Key", type='password')

if openai_api_key:
    st.info("OpenAI API Key set successfully.")
if pinecone_api_key:
    st.info("Pinecone API Key set successfully.")

# Initialize OpenAI and Pinecone clients
openai_client = OpenAI(api_key=openai_api_key)
if(pinecone_api_key):
    pinecone = Pinecone(api_key=pinecone_api_key)

# File Uploader
file = st.file_uploader("Upload a PDF file", type=["pdf"])


if file:
    if(not pinecone_api_key):
        st.info("Please enter Pinecone API Key.")
        
    pdf_reader = PdfReader(file)
    pdf_text = ""
    for page_number in range(len(pdf_reader.pages)):
        pdf_text += pdf_reader.pages[page_number].extract_text()
    resume = pdf_text

    # Embed text into Pinecone
    timestamp = time.time()
    xcs = get_embeddings(resume).data[0].embedding
    ids = ["1"]
    metadatas = [{"text": resume}]
    records = [(str(timestamp), xcs, metadatas[0])]
    index_name = "langchainvector"
    index = pinecone.Index(index_name)
    index.delete(delete_all=True)
    index.upsert(vectors=records)
    st.success("Resume uploaded and embedded into Pinecone.")

    # User Query
    st.header("Ask a Question")
    query = st.text_input("Ask a question about the resume:")

    if query:
        embed = get_embeddings(query)
        res = index.query(vector=embed.data[0].embedding, top_k=3, include_metadata=True)
        contexts = [x["metadata"]["text"] for x in res["matches"]]

        # GPT-3.5 Turbo Prompt
        prompt_start = (
            "Answer the question based on the context below. You are provided with a resume about a person whose name is mentioned. Sell the skills mentioned in the resume and avoid making up information if they are not provided exactly. \n\n"
            "Context:\n"
        )
        prompt_end = f"\n\nQuestion: {query}\nAnswer:"
        prompt = prompt_start + "\n\n---\n\n".join(contexts) + prompt_end

        # Call GPT-3.5 Turbo
        gpt_response = openai_client.completions.create(
            model="gpt-3.5-turbo-instruct",
            prompt=prompt,
            temperature=0,
            max_tokens=1500,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0,
            stop=None,
        )

        # Display GPT-3.5 Turbo response
        st.header("GPT-3.5 Turbo Response")
        st.info("Below is the response generated by GPT-3.5 Turbo based on the provided context.")
        st.write(gpt_response.choices[0].text)
